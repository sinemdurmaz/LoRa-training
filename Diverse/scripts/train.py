# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1meUpey4Y0jACD0MQYHuRBH4wWv84tPCZ
"""

!pip install -q -U torch transformers peft datasets==2.19.2 trl accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# GÃ¶rseldeki Model ID'si
model_id = "Qwen/Qwen2.5-Coder-1.5B-Instruct"

print(f"{model_id} modeli bfloat16 formatÄ±nda yÃ¼kleniyor...")

# Modeli YÃ¼kleme (Quantization olmadan)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,       # L4 GPU iÃ§in en verimli ve kararlÄ± format
    device_map="auto"           # Otomatik olarak GPU'ya yerleÅŸtir
)

# Tokenizer YÃ¼kleme
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Qwen modellerinde bazen pad_token tanÄ±mlÄ± gelmeyebiliyor,
# eÄŸitimde hata almamak iÃ§in eos_token'Ä± pad_token olarak atÄ±yoruz.
tokenizer.pad_token = tokenizer.eos_token

model.config.use_cache = False

print("Model ve Tokenizer (Quantization olmadan) baÅŸarÄ±yla yÃ¼klendi!")

# 3. Model Testi (Inference)

# Sormak istediÄŸin soru
prompt = "Prepare a code to sum two input value."

# Modeli sohbet formatÄ±na hazÄ±rlama (Qwen prompt ÅŸablonunu uygular)
messages = [
    {"role": "system", "content": "You are an expert Python programmer. Please read the problem carefully before wr1t1ng any Python code."},
    {"role": "user", "content": prompt}
]

# Promptu modelin anlayacaÄŸÄ± formata (token) Ã§evirme
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

print("Cevap Ã¼retiliyor...\n" + "-"*30)

# CevabÄ± Ã¼retme
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,  # CevabÄ±n uzunluk limiti
    do_sample=True,      # YaratÄ±cÄ± Ã¼retim iÃ§in (False yaparsan hep aynÄ± cevabÄ± verir)
    temperature=0.7      # YaratÄ±cÄ±lÄ±k katsayÄ±sÄ±
)

# Sadece Ã¼retilen yeni kÄ±smÄ± alÄ±p yazÄ±ya Ã§evirme (decode)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

print(response)

from datasets import load_dataset

# -------------------------------------------------
# 1. Dataset YÃ¼kleme
# -------------------------------------------------

dataset_name = "Naholav/CodeGen-Deep-5K"
print(f"{dataset_name} indiriliyor...")

# Dataset sadece train split iÃ§eriyor
full_dataset = load_dataset(dataset_name, split="train")

print(f"Toplam Veri: {len(full_dataset)}")

# -------------------------------------------------
# 2. Train / Validation / Test BÃ¶lme (80 / 10 / 10)
# -------------------------------------------------

# Ã–nce %80 Train, %20 Temp (Val + Test)
split_1 = full_dataset.train_test_split(
    test_size=0.2,
    seed=42
)

train_dataset = split_1["train"]   # %80
temp_dataset  = split_1["test"]    # %20

# Temp'i %10 Validation + %10 Test olarak bÃ¶l
split_2 = temp_dataset.train_test_split(
    test_size=0.5,
    seed=42
)

val_dataset  = split_2["train"]    # %10
test_dataset = split_2["test"]     # %10

print(f"EÄŸitim (Train): {len(train_dataset)}")
print(f"DoÄŸrulama (Validation): {len(val_dataset)}")
print(f"Test: {len(test_dataset)}")

# -------------------------------------------------
# 3. INSTRUCTION FORMATLAMA
# -------------------------------------------------

def format_instruction(sample):
    """
    Dataset'i Qwen / Instruct SFT formatÄ±na Ã§evirir.
    SADECE input + solution kullanÄ±lÄ±r.
    """
    return {
        "text": (
            "### Instruction:\n"
            f"{sample['input']}\n\n"
            "### Response:\n"
            f"{sample['solution']}"
        )
    }

print("Dataset formatlanÄ±yor...")

# Eski sÃ¼tunlarÄ± SÄ°LEREK map ediyoruz
train_dataset = train_dataset.map(
    format_instruction,
    remove_columns=train_dataset.column_names
)

val_dataset = val_dataset.map(
    format_instruction,
    remove_columns=val_dataset.column_names
)

test_dataset = test_dataset.map(
    format_instruction,
    remove_columns=test_dataset.column_names
)

# 4. KONTROL
# -------------------------------------------------

print("\n--- Ä°ÅLENMÄ°Å TRAIN Ã–RNEÄÄ° ---")
print(train_dataset[0]["text"])

from peft import LoraConfig, get_peft_model, TaskType

# 5. LoRA KonfigÃ¼rasyonu (EÄŸitim AyarlarÄ±)

peft_config = LoraConfig(
    r=16,           # Rank: EÄŸitilecek parametrelerin boyutu. (8, 16, 32, 64 kullanÄ±lÄ±r. 16 iyi bir baÅŸlangÄ±Ã§tÄ±r.)
    lora_alpha=32,  # Alpha: Ã–ÄŸrenme katsayÄ±sÄ± (Genelde r'nin 2 katÄ± seÃ§ilir).
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj", # Attention katmanlarÄ±
        "gate_proj", "up_proj", "down_proj"     # MLP katmanlarÄ±
    ], # Qwen modelindeki tÃ¼m lineer katmanlarÄ± hedefliyoruz (En iyi performans iÃ§in).
    lora_dropout=0.05, # AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) engellemek iÃ§in rastgele nÃ¶ron kapatma oranÄ±.
    bias="none",
    task_type=TaskType.CAUSAL_LM # Modelin tÃ¼rÃ¼ (Metin Ã¼retimi)
)

# AyarlarÄ± modele uygulama
model = get_peft_model(model, peft_config)

# Ne kadar parametre eÄŸiteceÄŸimizi gÃ¶relim
print("EÄŸitilecek parametre tablosu:")
model.print_trainable_parameters()

from transformers import TrainingArguments
from trl import SFTTrainer
from transformers import DataCollatorForLanguageModeling

# Kodun geri kalanÄ±nda 'eval_dataset' kullanÄ±ldÄ±ÄŸÄ± iÃ§in val_dataset'i atÄ±yoruz
eval_dataset = val_dataset

model.gradient_checkpointing_enable()
model.config.use_cache = False

training_args = TrainingArguments(
    output_dir="qwen25_coder_15b_l4",

    num_train_epochs=3,

    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,   # efektif batch = 16

    learning_rate=4e-4,
    warmup_ratio=0.03,

    fp16=False,
    bf16=True,

    optim="adamw_torch_fused",
    weight_decay=0.01,
    max_grad_norm=0.3,

    save_strategy="steps",
    save_steps=100,
    logging_steps=20,

    eval_strategy="steps",
    eval_steps=100,
    # eval_dataset parametresi buradan kaldÄ±rÄ±ldÄ±. Trainer'a verilecek.

    load_best_model_at_end=False,
    lr_scheduler_type="cosine",

    report_to="tensorboard",
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

tokenizer.model_max_length = 1024

def tokenize_fn(example):
    return tokenizer(
        example["text"],
        truncation=True,
        max_length=1024,
        padding=False,
    )

# Check if 'text' column exists before mapping to avoid KeyError on re-run
if "text" in train_dataset.column_names:
    print("Tokenizing train dataset...")
    train_dataset = train_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=train_dataset.column_names,
    )
else:
    print("Train dataset already tokenized.")

if "text" in eval_dataset.column_names:
    print("Tokenizing eval dataset...")
    eval_dataset = eval_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=eval_dataset.column_names,
    )
else:
    print("Eval dataset already tokenized.")


# Trainer'a eval dataseti de veriyoruz
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    data_collator=data_collator,
)

# Gradient checkpointing kullanÄ±ldÄ±ÄŸÄ± iÃ§in input embedding'lerin gradyan gerektirdiÄŸini belirtmeliyiz.
# Bu iÅŸlem yapÄ±lmazsa "element 0 of tensors does not require grad" hatasÄ± alÄ±nÄ±r.
if hasattr(model, "enable_input_require_grads"):
    model.enable_input_require_grads()

trainer.train()

import os
import gc
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, DataCollatorForLanguageModeling

# ----------------------------------------
# GPU / VRAM TEMÄ°ZLÄ°K (TRAINING SONRASI)
# ----------------------------------------

def clear_vram():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

print("ğŸ§¹ Training sonrasÄ± VRAM temizleniyor...")
clear_vram()

# ----------------------------------------
# Ayarlar
# ----------------------------------------

# EÄÄ°TÄ°MDEKÄ° output_dir Ä°LE AYNI OLMALI
output_dir = "qwen25_coder_15b_l4"
device = "cuda" if torch.cuda.is_available() else "cpu"

best_checkpoint = None
best_test_loss = float("inf")

# ----------------------------------------
# Tokenizer (tek sefer)
# ----------------------------------------

# Tokenizer eÄŸitim Ã§Ä±ktÄ±sÄ±nda yoksa, orijinal modelden yÃ¼klenir
try:
    tokenizer = AutoTokenizer.from_pretrained(output_dir)
except:
    print(f"{output_dir} iÃ§inde tokenizer bulunamadÄ±, orijinal modelden yÃ¼kleniyor...")
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")

tokenizer.pad_token = tokenizer.eos_token

# ----------------------------------------
# Test Verisini Tokenize Et
# ----------------------------------------

def tokenize_fn(example):
    return tokenizer(
        example["text"],
        truncation=True,
        max_length=1024,
        padding=False,
    )

# test_dataset globalden geliyor, tokenize edilmemiÅŸ olabilir.
if "input_ids" not in test_dataset.column_names:
    print("Test verisi tokenize ediliyor...")
    test_dataset = test_dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=test_dataset.column_names
    )
else:
    print("Test verisi zaten tokenize edilmiÅŸ.")

# ----------------------------------------
# Checkpoint'leri Bul
# ----------------------------------------

if not os.path.exists(output_dir):
    print(f"HATA: {output_dir} klasÃ¶rÃ¼ bulunamadÄ±. LÃ¼tfen eÄŸitimin tamamlandÄ±ÄŸÄ±ndan emin olun.")
    checkpoints = []
else:
    checkpoints = sorted(
        [
            os.path.join(output_dir, d)
            for d in os.listdir(output_dir)
            if d.startswith("checkpoint-")
        ],
        key=lambda x: int(x.split("-")[-1])
    )

print(f"Bulunan checkpoint sayÄ±sÄ±: {len(checkpoints)}")

# ----------------------------------------
# CHECKPOINT â†’ TEST EVAL LOOP (VRAM SAFE)
# ----------------------------------------

for ckpt in checkpoints:
    print(f"\nğŸ” Test ediliyor: {ckpt}")

    clear_vram()  # gÃ¼venlik amaÃ§lÄ±, yÃ¼klemeden Ã¶nce

    model = AutoModelForCausalLM.from_pretrained(
        ckpt,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True
    )

    # Loss hesabÄ± iÃ§in data_collator gerekli (Labels oluÅŸturur)
    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        eval_dataset=test_dataset,
        data_collator=data_collator
    )

    with torch.no_grad():
        metrics = trainer.evaluate()

    if "eval_loss" in metrics:
        test_loss = metrics["eval_loss"]
        print(f"ğŸ“‰ Test Loss: {test_loss}")

        if test_loss < best_test_loss:
            best_test_loss = test_loss
            best_checkpoint = ckpt
    else:
        print("âš ï¸ Eval loss hesaplanamadÄ±. (Metrics iÃ§inde 'eval_loss' yok)")
        print("Metrics:", metrics)

    # ---- VRAM TEMÄ°ZLÄ°ÄÄ° (EN Ã–NEMLÄ° KISIM) ----
    del trainer
    del model
    clear_vram()

# ----------------------------------------
# SONUÃ‡
# ----------------------------------------

if best_checkpoint:
    print("\nğŸ† FINAL CHECKPOINT SEÃ‡Ä°LDÄ°")
    print(f"Checkpoint: {best_checkpoint}")
    print(f"En Ä°yi Test Loss: {best_test_loss}")
else:
    print("\nâš ï¸ HiÃ§bir checkpoint test edilemedi.")

# ===============================
# CELL 1: TRAIN / VAL LOG KAYDI
# ===============================

import os
import json
import shutil

# ---- Ayarlar ----
LOCAL_LOG_DIR = "logs"
DRIVE_LOG_DIR = "/content/drive/MyDrive/qwen_logs"
OUTPUT_DIR = "qwen25_coder_15b_l4"

os.makedirs(LOCAL_LOG_DIR, exist_ok=True)

# ---- Log GeÃ§miÅŸini Dosyadan Okuma ----
# Trainer nesnesi silindiÄŸi iÃ§in logs verisini son checkpoint'teki dosyadan okuyoruz.
log_history = []
try:
    if os.path.exists(OUTPUT_DIR):
        checkpoints = sorted(
            [d for d in os.listdir(OUTPUT_DIR) if d.startswith("checkpoint-")],
            key=lambda x: int(x.split("-")[-1])
        )

        if checkpoints:
            last_ckpt = checkpoints[-1]
            state_path = os.path.join(OUTPUT_DIR, last_ckpt, "trainer_state.json")

            if os.path.exists(state_path):
                with open(state_path, "r") as f:
                    state_data = json.load(f)
                log_history = state_data.get("log_history", [])
                print(f"âœ… Log geÃ§miÅŸi {state_path} dosyasÄ±ndan okundu.")
            else:
                print("âš ï¸ trainer_state.json bulunamadÄ±.")
        else:
            print("âš ï¸ HiÃ§bir checkpoint bulunamadÄ±.")
    else:
        print(f"âš ï¸ {OUTPUT_DIR} klasÃ¶rÃ¼ bulunamadÄ±.")
except Exception as e:
    print(f"âš ï¸ Log okuma hatasÄ±: {e}")

# ---- JSON Olarak Kaydetme ----
train_val_log_path = f"{LOCAL_LOG_DIR}/training_validation_logs.json"

with open(train_val_log_path, "w") as f:
    json.dump(log_history, f, indent=2)

print(f"âœ… Training + Validation loglarÄ± kaydedildi: {train_val_log_path}")

# ---- Google Drive'a kopyala (Colab ise) ----
try:
    from google.colab import drive
    if not os.path.exists("/content/drive"):
        drive.mount("/content/drive")

    os.makedirs(DRIVE_LOG_DIR, exist_ok=True)
    shutil.copy(train_val_log_path, DRIVE_LOG_DIR)

    print(f"â˜ï¸ Drive'a kopyalandÄ±: {DRIVE_LOG_DIR}")
except:
    print("â„¹ï¸ Drive ortamÄ± yok veya eriÅŸilemedi, atlandÄ±.")

# =====================================================
# CELL 2: CHECKPOINT TEST + JSON + GRAFÄ°K + DOWNLOAD
# =====================================================

import os
import gc
import json
import torch
import shutil
import matplotlib.pyplot as plt
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, DataCollatorForLanguageModeling

# ---------- AYARLAR ----------
OUTPUT_DIR = "qwen25_coder_15b_l4"
LOCAL_LOG_DIR = "logs"
DRIVE_LOG_DIR = "/content/drive/MyDrive/qwen_logs"

os.makedirs(LOCAL_LOG_DIR, exist_ok=True)

def clear_vram():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

# ---------- CHECKPOINT BUL ----------
checkpoints = sorted(
    [os.path.join(OUTPUT_DIR, d) for d in os.listdir(OUTPUT_DIR) if d.startswith("checkpoint-")],
    key=lambda x: int(x.split("-")[-1])
)

# Tokenizer yÃ¼kleme (Hata Ã¶nleyici fallback yapÄ±sÄ±)
try:
    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)
except:
    print(f"{OUTPUT_DIR} iÃ§inde tokenizer bulunamadÄ±, orijinal modelden yÃ¼kleniyor...")
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")

tokenizer.pad_token = tokenizer.eos_token

test_results = []
best_ckpt, best_loss = None, float("inf")

clear_vram()

# ---------- TEST LOOP ----------
for ckpt in checkpoints:
    print(f"ğŸ” Test ediliyor: {ckpt}")
    clear_vram()

    model = AutoModelForCausalLM.from_pretrained(
        ckpt,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        low_cpu_mem_usage=True
    )

    # Loss hesabÄ± iÃ§in gerekli
    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        eval_dataset=test_dataset,
        data_collator=data_collator
    )

    with torch.no_grad():
        metrics = trainer.evaluate()

    # EÄŸer loss varsa al, yoksa uyarÄ± ver
    if "eval_loss" in metrics:
        loss = metrics["eval_loss"]
    else:
        loss = float("inf")
        print(f"âš ï¸ {ckpt} iÃ§in eval_loss hesaplanamadÄ±!")

    test_results.append({
        "checkpoint": ckpt,
        "step": int(ckpt.split("-")[-1]),
        "test_loss": loss
    })

    if loss < best_loss:
        best_loss = loss
        best_ckpt = ckpt

    del model, trainer
    clear_vram()

# ---------- JSON KAYIT ----------
test_log_path = f"{LOCAL_LOG_DIR}/test_checkpoint_results.json"
with open(test_log_path, "w") as f:
    json.dump(test_results, f, indent=2)

print(f"\nğŸ† FINAL CHECKPOINT: {best_ckpt}")
print(f"ğŸ“‰ En iyi Test Loss: {best_loss}")
print(f"âœ… Test loglarÄ± kaydedildi: {test_log_path}")

# ---------- GRAFÄ°KLER ----------
with open(f"{LOCAL_LOG_DIR}/training_validation_logs.json") as f:
    logs = json.load(f)

train_s, train_l, eval_s, eval_l = [], [], [], []
for x in logs:
    if "loss" in x:
        train_s.append(x["step"])
        train_l.append(x["loss"])
    if "eval_loss" in x:
        eval_s.append(x["step"])
        eval_l.append(x["eval_loss"])

plt.figure()
plt.plot(train_s, train_l, label="Train")
plt.plot(eval_s, eval_l, label="Validation")
plt.legend()
plt.title("Train / Validation Loss")
plt.show()

steps = [x["step"] for x in test_results]
losses = [x["test_loss"] for x in test_results]

# 1. Benchmark reposunu Ã§ekiyoruz
!git clone https://github.com/naholav/CodeGen.git

# 2. Gerekli kÃ¼tÃ¼phaneleri kuruyoruz
!pip install -r CodeGen/requirements.txt

import os
import shutil
import json
import math

# --- AYARLAR ---
# Senin eÄŸitimin Ã§Ä±ktÄ±ÄŸÄ± klasÃ¶r
source_dir = "./qwen25_coder_15b_l4"

# PDF'teki hedef klasÃ¶r (Hangi eÄŸitimi yapÄ±yorsan ismini ona gÃ¶re dÃ¼zelt: 'deep' veya 'diverse')
target_base = "./CodeGen/models/deep_instruction/checkpoints"

print(f"ğŸ“‚ Dosyalar '{source_dir}' kaynaÄŸÄ±ndan '{target_base}' hedefine taÅŸÄ±nÄ±yor...\n")

if not os.path.exists(target_base):
    os.makedirs(target_base)

# Checkpointleri bul
for folder_name in os.listdir(source_dir):
    if "checkpoint-" in folder_name:
        src_path = os.path.join(source_dir, folder_name)

        # 1. trainer_state.json dosyasÄ±nÄ± bul ve oku
        json_path = os.path.join(src_path, "trainer_state.json")

        real_epoch = 1 # VarsayÄ±lan deÄŸer (dosya bulunamazsa)

        if os.path.exists(json_path):
            with open(json_path, 'r') as f:
                data = json.load(f)
                # Epoch deÄŸerini al (Ã¶rn: 2.14) ve yukarÄ± yuvarla (Ã¶rn: 3)
                # Ã‡Ã¼nkÃ¼ 2.14 demek, 2 bitmiÅŸ 3. epoch'un iÃ§indeyiz demektir.
                if 'epoch' in data:
                    real_epoch = math.ceil(data['epoch'])

        # 2. AdÄ± PDF formatÄ±na Ã§evir
        # Format: checkpoint-step-{ADIM}-epoch-{GERÃ‡EK_EPOCH}
        step_num = folder_name.split("-")[-1] # "checkpoint-500" -> "500"
        new_folder_name = f"checkpoint-step-{step_num}-epoch-{real_epoch}"

        dest_path = os.path.join(target_base, new_folder_name)

        # 3. Kopyalama iÅŸlemi
        if os.path.exists(src_path):
             # EÄŸer hedefte zaten varsa hata vermemesi iÃ§in dirs_exist_ok=True
             shutil.copytree(src_path, dest_path, dirs_exist_ok=True)
             print(f"âœ… Ä°ÅŸlendi: {folder_name} -> {new_folder_name} (Epoch verisi json'dan okundu)")

print("\nğŸ KlasÃ¶r dÃ¼zeni ve isimlendirme tamamlandÄ±!")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/CodeGen
!pip install flash-attn --no-build-isolation
!python livecodebench_eval.py \
  --model_type deep_instruction \
  --platform atcoder \
  --difficulty easy

!python livecodebench_eval.py --include_base --platform atcoder --difficulty easy